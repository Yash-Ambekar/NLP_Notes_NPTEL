-  Before touching upon Beam Search Algorithm, lets see how to create features given the context history of words and their POS tags.
![[Pasted image 20230707144910.png]]

- Example of Features
![[Pasted image 20230707144951.png]]
-  We use prefix and suffix to create feature set for rare words which may not occur in the test data but will help in-case of tagging other unknown words which we didn't encounter during training of the model.


### POS tagging
-  Given a sentence {$w_1, ..., w_n$} ,  a tag sequence candidate {$t1,...,t_n$}  has conditional probability:
$$
\begin{split}
P(t_1,...,t_n \ | \ w_1,...,w_n) &= \prod_\limits{i = 1}^{n} P(t_i | x_i)\\\\
where \ x_i \ is \ the \ context \ for \ &each \ individual \ word 
\end{split}
$$

##### Algorithm
-  Let W = {$w_1,..., w_n$}  be a test sentence,  $s_{ij}$  be the $j^{th}$  highest probability tag sequence up to and including word $w_i$. (here $j$  is the probability ranking of each tag sequence).
-  Generate tags for $w_1$ , find top $N$  tags using MaxEnt model,  set  $t_{1j}$  ,  $1 \le \ j \ \le \ N$ , accordingly.
- Initialize loop $i = 2$
	- Initialize loop $j = 1$
	- Generate tags for $w_i$ , given $t_{(i-1)j}$  as previous tag context, and append each tag to  $t_{(i-1)j}$ to make a new sequence
	- $j = j+1$, repeat if $j \ \le \ N$
-  Find $N$  highest probability sequences generated by above loop, set $s_{ij}$  accordingly.
-  $i = i+1$,  repeat if $i \ \le \ n$
- Return highest probability sequence $t_{n1}$ 


### **Note**:  
- Maximum Entropy Model is a general classifier for finding the $P( \ tag \ | \ word \ )$ for each word individually.
- This application of Beam Search Algorithm is called **Maximum Entropy Markov Model.** 


### Example Question
![[Pasted image 20230708150835.png]]
![[Pasted image 20230708152131.png]]

-  So we have to design a MaxEnt tagger using the Beam Search Algorithm with a beam-size of 2 i.e. only the top 2  highest probability tag sequences are taken into consideration.
-  $h_i$  is the given context history for word  $w_i$ , 
$$h_{ij} = \{ w_i, w_{i-1}, w_{i+1}, (t_{i-1})_j \}$$
$w_{i+1}$ ---> Next word 
$w_{i-1}$  ---> Previous word
$(t_{i-1})_j$  ---> $j^{th}$  previous word's tag.

-  There are total 8 features ($f_{1 \ \le \ i \ \le \ 8}$) and it is given that each feature has uniform weight i.e.  $\lambda_{1 \ \le \ i \ \le \ 8} = 1$ .

-  We know that,
$$P_\lambda(t|w) = \frac{1}{Z_{\lambda}(w)} \cdot exp \; \{\; \sum\limits_{i = 1}^N \lambda_i f_i(w,t)\; \}$$
$$Z_{\lambda}(w) = \sum\limits_{t} exp \; \{\; \sum\limits_{i = 1}^N \lambda_i f_i(w,t)\; \}$$

-  For start of the sentence, $w_i = the$  and  we have two tags  $DET$  and  $Noun$   therefore  $P \ (\ Det \ | \ h_{11} = \{the, \ NULL, \ light, \ NULL \} \ )$  and  $P \ (\ Noun \ | \ h_{12} = \{the, \ NULL, \ light, \ NULL \} \ )$  will be equal to,

$$
\begin{split}
P \ (\ Det \ | \ h_{11} \ ) &= \frac{e^{\sum\limits_{i = 1}^8 \lambda_i f_i( \ h_{11},\ Det)}}{Z} \\\\
\lambda_{1 \ \le \ i \ \le \ 8} = 1 \ and \ &for \ Det \ only \ the \ 7^{th} feature \ is \ valid  \\\\
e^{\sum\limits_{i = 1}^8 \lambda_i f_i( \ h_{11},\ Det)} &= e^{1 (0+0+0+0+0+0+1+0)} = e^1\\\\
for \  tag \  & Noun, \ 8^{th} \ feature \ is \ valid\\\\
e^{\sum\limits_{i = 1}^8 \lambda_i f_i( \ h_{12},\ Noun)} &= e^{1 (0+0+0+0+0+0+0+1)} = e^1\\\\
Z &= Normalizing \ factor \\\\
&= e^1 + e^1 = 2 \times e \\\\
\therefore \ P \ (\ Det \ | \ h_{11} \ ) &= \frac{e}{2e} = 0.5 \\\\
Similarly, \ P \ (\ Noun \ | \ h_{12} \ ) &= \frac{e}{2e} = 0.5
\end{split}
$$
 
-  Beam size is 2 therefore we keep both the tags.


- We now move onto the next word  $light$  and make tag sequences using the previous two tags  $Det$ ,  $Noun$  and current two tags  $Adj$  and  $Verb$   i.e.  $DA , DV, NA, NV$ .  We find the top 2 most probable sequence out of these 4 for computing the next most probable tag for the sequence.
-  So,
$$P( \ DA \ | \ h_{21} \ ) = P(Det \ | \ h_{11}) \ \times \ P(Adj \ | \ h_{21})$$
$$P( \ DV \ | \ h_{21} \ ) = P(Det \ | \ h_{11}) \ \times \ P(Verb \ | \ h_{21})$$
$$P( \ NA \ | \ h_{22} \ ) = P(Noun \ | \ h_{12}) \ \times \ P(Adj \ | \ h_{22})$$
$$P( \ NV \ | \ h_{21} \ ) = P(Noun \ | \ h_{12}) \ \times \ P(Verb \ | \ h_{22})$$

where  $h_{21} = \{light, \ the, \ book, \ Det \}$ and  $h_{22} = \{light, \ the, \ book, \ Noun \}$.

-  Now we have to calculate the $P(Adj \ | \ h_2)$  using the same process that we did for computing  $P \ (\ Det \ | \ h_1 \ )$ .
$$\begin{split}
P \ (\ Adj \ | \ h_{21} \ ) &= \frac{e^{\sum\limits_{i = 1}^8 \lambda_i f_i( \ h_{21},\ Adj)}}{Z} \\\\
for \ Adj \ and \ Det \  only \ & the \ 1^{st}, \ 4^{th} \ and \ 5^{th}\ feature \ is \ valid  \\\\
e^{\sum\limits_{i = 1}^8 \lambda_i f_i( \ h_{21},\ Adj)} &= e^{1 (1+0+0+1+1+0+0+0)} = e^3 \\\\
for \  tag \   Verb \ and  \ & Det, \ none \ of \ the \ features \ are \ valid\\\\
e^{\sum\limits_{i = 1}^8 \lambda_i f_i( \ h_{21},\ Verb)} &= e^{1 (0+0+0+0+0+0+0+0)}  = e^0\\\\
Z &= Normalizing \ factor \\\\
&= e^3+e^0 = e^3 + 1 \\\\
\because \ P \ (\ Adj \ | \ h_{21} \ ) &= \frac{e^3}{e^3 + 1} \\\\
\therefore \ P( \ DA \ | \ h_{21} \ ) &= 0.5 \ \times \ \frac{e^3}{e^3 + 1} \\\\
Also,\ P \ (\ Verb \ | \ h_{22} \ ) &= \frac{1}{e^3 + 1} \\\\
\therefore \ P(DV \ | \ h_{22}) &= 0.5 \ \times \ \frac{1}{e^3 + 1} 
\end{split} $$

-  Similarly we calculate  $P( \ DA \ | \ h_{21} \ )$ , $P( \ DV \ | \ h_{21} \ )$ , $P( \ NA \ | \ h_{22} \ )$  and $P( \ NV \ | \ h_{22} \ )$ and select the top 2 highest probabilities.

-  Suppose, the selected tag sequences are $DA$  and  $NV$  then we have to move to the next word and find the probabilities $P( \ DAN \ | \ h_{31} \ )$ , $P( \ DAV \ | \ h_{31} \ )$ ,  $P( \ NVN \ | \ h_{32} \ )$  and  $P(NVV | h_{32})$ .
  where,
   $h_{31} = \{book, \ light, \ NULL, \ Adj \}$   and  $h_{32} = \{book, \ light, \ NULL, \ Verb \}$
   
$$P( \ DAN \ | \ h_{31} \ ) = P(DA \ | \ h_{21}) \ \times \ P(Noun \ | \ h_{31})$$
$$P( \ DAV \ | \ h_{31} \ ) = P(DA \ | \ h_{21}) \ \times \ P(Verb \ | \ h_{31})$$
$$P( \ NVN \ | \ h_{32} \ ) = P(NV \ | \ h_{22}) \ \times \ P(Noun \ | \ h_{32})$$
$$P( \ NVV \ | \ h_{32} \ ) = P(NV \ | \ h_{22}) \ \times \ P(Verb \ | \ h_{32})$$

-  Max of these probabilities will get us the correct sequence.

- This approach has a small issue which is solved by the Conditional Random Fields   [[6. Conditional Random Fields]].


### References
-  [NPTEL NLP Course]([https://youtu.be/LFuuNYF7Pc0](https://youtu.be/LFuuNYF7Pc0))
-  [Foundations of NLP Explained Visually: Beam Search, How It Works by Ketan Doshi](https://towardsdatascience.com/foundations-of-nlp-explained-visually-beam-search-how-it-works-1586b9849a24)
