
-  MaxEnt is a Discriminative Type of Model compared to the HMM which is a Generative Model.  
-  In this model, we may identify a heterogenous set of features which contribute in some way to the choice of POS tag of the current word. For Eg:
	- Whether a word is the first word in the article.
	-  Whether the next word is $to$ .
	- Whether one of the last 5 words is a preposition, etc.
- MaxEnt combines these features in a probabilistic model.

### Intuition
-  Model all that is known and assume nothing about that which is unknown. 
- Given a collection of facts, choose a model which is consistent with all the facts, but otherwise as uniform or as uncertain as possible.
- Entropy measures the uncertainty of a distribution which is shown by this formula,
$$H(P) = \sum\limits_{x} P(x).log_2(P(x))$$
 $x$  is an event.
$P(x)$  is the probability of that event.
$log_2(P(x))$  comes from the uncertainty (surprise)  i.e.  $log_2(\frac{1}{P(x)})$  (for higher probability the entropy or the uncertainty of identifying it is low and vice versa).

-  In the graph below you can see that entropy is close to zero for events with high certainty that they would take place or not take place and the entropy is highest when probability is $0.5$  that is max uncertainty or surprise.
![[1024px-Mplwp_shannon_entropy.svg.png]]

### Prerequisite
-  Watch these videos
	-  [1. Maximum Entropy Tutorial: Intro to MaxEnt](https://www.youtube.com/watch?v=6YEn9QRy3ks&list=PLF0b3ThojznT3olRuplp5x41wUp_LZxHL&index=1&pp=iAQB)
	-  [2. Maximum Entropy Methods Tutorial: A Simple Example: The Taxicab](https://youtu.be/5P58wHbWXBU)
	-  [3. Maximum Entropy Methods Tutorial: The Maximum Entropy Method](https://youtu.be/uZoURtzcN9M)
	- [4. Maximum Entropy Methods Tutorial: MaxEnt applied to Taxicab Example Part 1](https://youtu.be/kw4deNTVO7A)
	- [5. Maximum Entropy Methods Tutorial: MaxEnt applied to Taxicab Example Part 2](https://youtu.be/N9aWmecOJxQ)


- So we have a set of constraints or features that our model or the probability distribution has to satisfy. Lets say the features are denoted by  $f_i$  where  $i \; \epsilon$  { $1, 2,.....,n$}.
- Let the observation(word) be denoted as  $w$  and the class (POS tag) it belongs to be  $t$  and the sequence of word be $W$  and set of possible POS tags be $T$ .
-  MaxEnt model gives us a formulation for calculating the probability of  $t$  to occur given  $w$,
$$\boxed{P_\lambda(t|w) = \frac{e^{\sum\limits_{i} \lambda_i f_i(w,t)}}{Z_\lambda(x)}}$$
where $Z_\lambda(x)$  is a normalizing constant given by,
$$\boxed{Z_\lambda(w) = \sum\limits_{t} e^{\sum\limits_{i} \lambda_i f_i(w,t)}}$$

$\lambda_i$  is a weight given to a feature  $f_i$
-  The features are basically binary-valued indicator function such as below,
  $$
f(w,t)=
\begin{cases}
1, \; \; if \ isCapitalized(w) \ and \ t = NNP(noun)\\
0, \; \; otherwise
\end{cases}
$$

-  The expectation or the empirical count of a feature  is the average number of times $w$ and  $t$  occurred and it satisfied the feature function $f_i$ .
$$<fi> = \sum\limits_{w,t} P(w, t) \cdot f_i(w,t)$$
-  This value is also called as a Constraint that the MaxEnt model has to satisfy.
-  Refer this from the second video (timestamp - 5:00).
- Applying conditional probability because ultimately we want to compute $P(y|x)$
$$<fi> = \sum\limits_{x,y} P(x) \cdot P(y|x) \cdot f_i(x,y)$$
-  Adding more of these constraints lowers the maximum entropy as the model becomes more certain of assigning a class(POS tag). 

-  We can calculate conditional entropy where  $H(T|W=w)$ is the Entropy of  T conditioned on  $W$ taking a certain value $w$.
$$H(T|W=w) = -\sum\limits_{t \epsilon T} P(T = t | W = w) \cdot log_2P(T = t | W = w)$$
-  And  $H(T|W)$  is the result of averaging $H(T|W = w)$  over all the possible values of $w$  that $W$  may take.
- We get,
$$
\begin{split}
H(T|W)  \equiv & \sum\limits_{w} P(w).H(T|W=w)\\\\
= & -\sum\limits_{w} P(w) . \sum\limits_{t \epsilon T} P(t|w) \cdot log_2P(t|w)
\end{split}
$$
$$\boxed{H(T|W)= -\sum\limits_{w,t}P(w).P(t|w).log_2P(t|w)}$$
$P(w)$  is used as weight for different words (observation).

### Optimization
- Maximize  $H(T|W)$ subject to constraints  $f_i(w,t)$,  this can be achieved by assigning weight to the gradient of constraint  w.r.t  conditional probability $P(w|t)$ over all the no. of constraints  $N$ . 
- These weights are called Lagrange Multipliers ($\lambda$).
$$\frac{\partial H}{\partial p(w|t)} = \sum\limits_{i = 1}^{N} \lambda_i \frac{\partial f_i(w,t)}{\partial P(w|t)}$$
-  **NOTE:**  All the probabilities should sum to $1$   i.e.  $\sum\limits_{t,w} P(t,w) = 1$ , so this will also contribute to the maximization of entropy.
- After taking partial derivative w.r.t $P(t|w)$ then we solve the resulting equation for getting the value of $P(t|w)$ .
- We get $P(t|w)$ as,
$$\boxed{P_\lambda(t|w) = \frac{1}{Z_{\lambda}(w)} \cdot exp \; \{\; \sum\limits_{i = 1}^N \lambda_i f_i(w,t)\; \} }$$
where $Z_{\lambda}(x)$  is a normalizing constant given by,
$$\boxed{Z_{\lambda}(w) = \sum\limits_{t} exp \; \{\; \sum\limits_{i = 1}^N \lambda_i f_i(w,t)\; \}}$$

### Example Question
![[Pasted image 20230707124234.png]]

-  We are given 6 constraints in the form of conditional probability and we have to make a MaxEnt model with appropriate model parameters that satisfies these constraints and is as uniform as possible otherwise.
- We are also given a hint that 6 features $f_{1 \ \le \ i \ \le \ 6}$ should make the analysis easier .
- So we first construct our features in the form Binary valued Indicator Function using the constraints,
	-  $word \ is \ a$  and $tag \ is \ D$  then  $f_1 = 1 \ otherwise \ 0$  
	- $word \ is \ man$  and $tag \ is \ N$  then $f_2 = 1 \ otherwise \ 0$  
	- $word \ is \ sleeps$  and $tag \ is \ V$ then $f_3 = 1 \ otherwise \ 0$  
- These are the first 3 features.

- We are given a set $V$  (not the tag $V$) which is our vocabulary that contains words $a, \ man, \ sleeps$  as well as additional words.
- Our next 3 features will be constructed using the last 3 constraints in the following way:
	-  $word \ \epsilon \ V' = \{V - \{a ,\ man, \ sleeps\} \}$  and  $tag \ is \ D$  then$f_1 = 1 \ otherwise \ 0$
	- $word \ \epsilon \ V' = \{V - \{a ,\ man, \ sleeps\} \}$  and  $tag \ is \ N$  then$f_1 = 1 \ otherwise \ 0$
	-  $word \ \epsilon \ V' = \{V - \{a ,\ man, \ sleeps\} \}$  and  $tag \ is \ V$  then$f_1 = 1 \ otherwise \ 0$

-  Each of the 6 features is assigned a $\lambda_i$ (model parameter), which determine the weight that each feature will get to output a probability distribution as close and uniform the given data. 

-  Now suppose  if we want to calculate $P(D \ | \ cat)$  using the 6 features and model parameters ,
$$
\begin{split}
P(D \ | \ cat) &= \frac{e^{\sum_\limits{i} \lambda_i f_i}}{Z}\\\\
\sum_\limits{i} \lambda_i f_i &= \lambda_1 . 0 + \lambda_2 . 0 + \lambda_3 . 0 + \lambda_4 . 1 + \lambda_5 . 0 + \lambda_6 . 0 \\\\
e^{\sum_\limits{i} \lambda_i f_i} &= e^{\lambda_4}\\\\
Now \ lets \ calculate \ & Z \ over \ all \ possible \ tags \ and \ the \ word \ cat,\\\\
Z &= \sum\limits_{t} e^ {\sum\limits_{i = 1}^N \lambda_i f_i(cat,t)\;} \\\\
&= e^{\lambda_4 + \lambda_5 + \lambda_6}\\\\
\therefore \ P(D \ | \ cat) &= \frac{e^{\lambda_4}}{e^{\lambda_4 + \lambda_5 + \lambda_6}}\\\\
\end{split}
$$
-  Similarly we compute these values for $P(N \ | \ laughs) = \frac{e^{\lambda_5}}{e^{\lambda_4 + \lambda_5 + \lambda_6}}$  and  
  $P(D \ | \ Man) = \frac{e^0}{e^{\lambda_2}} = \frac{1}{e^{\lambda_2}}$  .


-  We use Beam Search Algorithm [[5.  Beam Search Algorithm]] for POS tagging using the MaxEnt model.


### References
-  [NPTEL NLP Course ](https://youtu.be/u3924yvlWBo](https://youtu.be/u3924yvlWBo "Share link")
- [Maximum Entropy Methods by Professor Simon DeDeo of Indiana University](https://youtu.be/6YEn9QRy3ks)
