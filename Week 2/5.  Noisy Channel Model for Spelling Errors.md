- In this model, we use the Bayesian Theorem to calculate the highest probability of  $w$   given  $x$ .
	- $w$ --> correct word
	- $x$ --> misspelled word
	- Real Corpus -> text containing the spelling error.
	- Corrected Corpus -> corrected text.

-   80% of  errors are within edit distance  1.
-  Almost all errors are within edit distance 2.
-  25 - 40% of spelling errors are real words.

```mermaid
graph LR
W --Noisy Channel--> X
```

- Using Bayesian theorem we can use $P(x|w)$ to calculate $P(w|x)$  as shown below.  Here we can see that $P(x)$ is constant for all words in dictionary therefore it can be ignored as we are comparing the values between each word in a dictionary.

$$
\begin{split}
\overset{-}{w} &= \underset{w \; \epsilon \; V}{\operatorname{arg max}} P(w|x)\\\\
&= \underset{w \; \epsilon \; V}{\operatorname{arg max}} \frac{P(x|w) \cdot P(w)}{P(x)}\\\\
& Ignoring \ P(x) \ as \ it \ is \ constant \ and \ we \ are \ comparing \ probabilities\\\\
&= \underset{w \; \epsilon \; V}{\operatorname{arg max}} P(x|w) \cdot P(w)
\end{split}
$$

-  $del [x,y]$:  count ($xy$  typed as  $x$) - Probability is calculated as the frequency of  $xy$  typed as  $x$ in the real corpus divided by the frequency of  $xy$  in corrected corpus.
  
-  $ins [x,y]$ :  count ($x$  typed as  $xy$) - Probability is calculated as the frequency of  $x$ typed as  $xy$  in the real corpus divided by the frequency of  $x$  in corrected corpus.
  
-   $sub [x,y]$:  count ($x$  typed as  $y$) - Probability is calculated as the frequency of  $x$ typed as  $y$  in the real corpus divided by the frequency of  $x$  in corrected corpus.
  
-   $trans[x,y]$ :  count ($xy$  typed as  $yx$) - Probability is calculated as the frequency of  $xy$ typed as  $yx$  in the real corpus divided by the frequency of  $xy$  in corrected corpus.


## Example of non-word spelling error

### Without Context
$x$ ---> acress  and we have a candidate set containing all the possible correct spelling.

![[Pasted image 20230522185017.png]]
**Note** :  acres is included  twice because the probability of insertion of  $s$ is different for 
inserting it before the last  $s$  or  after the last  $s$ .

-  Now we calculate $P(x|w)$ and $P(w)$ .

![[Pasted image 20230522134559.png]]
**Note**:  $P(w)$  is the probability of the word to occur in the corpus. Here we can see that  **across** is the correct word without considering the context as it has the highest probability value  x 10$^9$.  


### Same example with Context
- Sentence --> "$....\ versatile \ acress \ whose$"
- Our candidate set consist of two of the highest probability valued 
  words = {$actress, across$}.
-  Suppose,
	$P(actress|versatile)$ = 0.000021,   $P(across | versatile)$= 0.000021.
	$P(whose|actress)$ = 0.001, $P(whose | across)$ = 0.000006.
	then,
	P("$versatile \ actress \ whose$") = 0.000021 x 0.001 = 210 x 10^-10.
	P("$versatile \ across \ whose$") = 0.000021 x 0.000006 = 1 x 10^-10.

P("$versatile \ actress \ whose$") >  P("$versatile \ across \ whose$")  therefore the correct spelling may be **actress** in this context.


## Noisy  channel  model  for  real-word  spelling  correction.

-  Given a sentence $X = w_1, w_2, w_3,....,w_m$ 
- Candidate ($w_1$) = {$w_1, w'_1, w''_1, w'''_1,....$}
- Candidate ($w_2$) = {$w_2, w'_2, w''_2, w'''_2,....$}
- Candidate ($w_3$) = {$w_3, w'_3, w''_3, w'''_3,....$}

Here in the sentence $X$, we do not know which word is incorrect as they all can come under real word spelling error.
So we have to maximize $P(W | X)$ by choosing  optimal sequence of words $W$.

![[Pasted image 20230705204631.png]]

Correct sequence is "two of the".

- If we try all possible sequences here then it will be  $4 \cdot 3 \cdot 4 = 48$   sequences, which is can be exponential. Therefore we select two words and keep them constant and only change the third word.
- By doing this we will have $4 + 3 + 4 = 11$  sequences, which is much better than exponential. 

**Note**: Here we are making a general assumption that a sentence does not have more than one error.

$$P(X | W) = P(w'_1 | w_1)  \cdot  P(w_2 | w_2)  \cdot  P(w_3  | w_3)$$

- $P(w'_1 | w_1)$ --> same as how we calculated individual word probability in without context section.
- $P(w_2 | w_2),  P(w_3 | w_3)$ -->  Average number of error per $10/100$ words also denoted as  'Î±'.

### References
- [Stanford  resource on Noisy Channel](https://web.stanford.edu/~jurafsky/slp3/B.pdf)
-  [NPTEL NLP Course](https://youtu.be/HmcolVdXVpE)



