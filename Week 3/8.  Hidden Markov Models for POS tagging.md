-  It is a Generative Language Model i.e. it uses classes (POS tags) to generate words (sentence).

#### Probabilistic Tagging
-  Suppose we are given $W = w_1, w_2,...., w_n$  in a corpus and we have to assign tags to these words  $T = t_1, t_2,....,t_n$
- Here we try to maximize the probability of sequence of tags occurring given the sequence of words $P(T|W)$
-  So we can use Bayes theorem to get $P(T|W)$ from probability of sequence of word occurring given sequence of tags.
$$
\begin{equation}
\begin{split}
T^* = \underset{T}{\operatorname{argmax}} P(T|W) \\\\
= \underset{T}{\operatorname{argmax}} \frac{P(W|T) \cdot P(T)}{P(W)}\\\\
= \underset{T}{\operatorname{argmax}} P(W|T) \cdot P(T)\\
\end{split}
\end{equation}
$$
-  Applying Chain Rule
$$= \underset{T}{\operatorname{argmax}} \prod\limits_{i} P(w_i | w_1,....,w_{i-1}, t_1,....,t_{i-1}) \cdot P(t_i|t_1,...., t_{i-1})$$
(Here $P(W)$ is ignored since it will common for all tags )

-  Again,  calculating these probabilities is not feasible therefore we make some assumptions. 
-  $P(w_i | w_1,....,w_{i-1}, t_1,....,t_{i-1})\thickapprox P(w_i|t_i)$   i.e. probability of a word appearing depends only on its own POS tag.
- Bigram assumption: Probability of a tag appearing depends only on the previous tag i.e.
  $P(t_i|t_1,...., t_{i-1}) \thickapprox P(t_i|t_{i-1})$
-  Using these assumptions we get,
$$T^* = \underset{T}{\operatorname{argmax}} \prod\limits_{i} P(w_i|t_i) \cdot P(t_i|t_{i-1})$$
-  And
$$\begin{split} 
P(t_i|t_{i-1}) = \frac{C(t_{i-1}, t_i)}{C(t_{i-1})}\\\\
P(w_i|t_i) = \frac{C(t_i, w_i)}{C(t_i)}
\end{split}$$

$C(a,b)$ --> count of  $b$  occurring after  $a$.
$C(b)$ --> Count of  $b$  in corpus.


##### Example
-  Below there is an ambiguity between  $race$  as  $verb$  or  $noun$  .
![[Pasted image 20230623140652.png]]

-  So we have to calculate probabilities for these two cases and compare them to get the most probable tag.

-  **Interpretation  - 1**   : $verb$  is the correct tag
$P(t_i|t_{i-1}) = P(VB|TO)$ 

Here we have to also take the tag of tomorrow into consideration as it is after the word race and it should depend on the tag of race as per our assumptions.
$P(t_{i+1}|t_{i}) = P(NR|VB)$  

$P(w_i|t_i) = P(race|VB)$ 

$$P_{overall for VB} = P(VB|TO) \cdot P(NR|VB) \cdot P(race|VB)$$

-  **Interpretation -2**:  $noun$  is the correct tag
$P(t_i|t_{i-1}) = P(NN|TO)$ 
$P(t_{i+1}|t_{i}) = P(NR|NN)$  
$P(w_i|t_i) = P(race|NN)$

$$P_{overall for NN} = P(NN|TO) \cdot P(NR|NN) \cdot P(race|NN)$$

-  In this case  $race$  should be a  $verb$  as per the given context. Therefore $P_{overall for VB} > P_{overall for NN}$   should be true.


-  This is the **Hidden Markov Model** where we calculated the probability of the word occurring given the tag of the word.
-  In this model the words are called observations which are emitted by the model given the tag states. 
 ![[Pasted image 20230623152149.png]]
-  $x_1, x_2....$  are the states i.e. POS tags  and  $y_1, y_2,....$  are the emissions i.e. words.

#### <font color = "green"> Why is it called "hidden"?</font>
-  Understanding what is a Markov Chain first takes priority before answering this question.

### Markov Chain
-  It is also called as the first-order Markov Model.
Lets take an example
![[Pasted image 20230623152843.png]]
-   As per our assumption $P(q_n|q_{n-1}, q_{n-2},..., q_1) \approx P(q_n|q_{n-1})$ i.e. the weather of next day depends on the weather of the current  day. (Here weathers are basically tags)
![[Pasted image 20230623200750.png]]

-  We can use this table by selecting what the whether is like today and then looking up the different probabilities for the next day’s weather. For example if today is sunny then there is a 20% chance of it being rainy tomorrow, 10% chance of it being cloudy, and 70% chance of it being sunny again.
  
Graph Representation
-  An alternative to the table-view is to think of the states and transitions as a graph:
![[Pasted image 20230623200848.png]]

**Q**. What is the probability that tomorrow is sunny and day after is rainy given that today is sunny?
$$P(q_{n+2} = rainy, q_{n + 1} = sunny | q_n = sunny) $$
-  Applying chain rule,
$$= P(q_{n+1} = sunny| q_n = sunny) \cdot P(q_{n+2} = rainy | q_{n+1} = sunny, q_n = sunny)$$
-  Because of first order Markov assumption, the weather of  $i+1^{th}$  day  should depend on the  weather of $i^{th}$  day.
$$P(q_{n+2} = rainy | q_{n+1} = sunny, q_n = sunny) = P(q_{n+2} = rainy | q_{n+1} = sunny)$$
-  Therefore,
$$P(q_{n+2} = rainy, q_{n + 1} = sunny | q_n = sunny) = P(q_{n+1} = sunny| q_n = sunny) \cdot P(q_{n+2} = rainy | q_{n+1} = sunny)$$
$$ P(q_{n+2} = rainy, q_{n + 1} = sunny | q_n = sunny) = 0.7 \cdot 0.2 = 0.14$$

- In Markov model the states (weather) are observed and they transition from one state to another. But in case of Hidden Markov Model the POS tags are hidden variable  (states) which are not observed and the emission from those POS tags are words which are observed. 
  ![[Pasted image 20230623210811.png]]
  ![[Pasted image 20230623211934.png]]

Graph with word emission
![[Pasted image 20230623212026.png]]


#### Example start of a sentence
![[Pasted image 20230623212212.png]]

-  To get the correct sequence of tags, one naïve way will be to calculate probabilities for all combinations of tag i.e. $2 * 1 * 4* 2 * 2 = 32$  possibilities. This might not seem that tedious but a sentence can contain 15 or 20 words, then it will be exponential to calculate these probabilities.
-  We use Viterbi Algorithm[[2.  Viterbi Decoding Algorithm]] for efficiently carrying out this task. 