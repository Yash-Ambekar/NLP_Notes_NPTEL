### <font color = "green"> What is Part of Speech (POS) tagging?  </font>
-  A category to which a word is assigned in accordance with its syntactic functions. In English the main parts of speech are noun, pronoun, adjective, determiner, verb, adverb, preposition, conjunction, and interjection.

##### Open class words (content words)
- Nouns, Verbs, Adjectives, Adverbs.
- They are mostly content bearing and they refer to objects, actions, and features in the world.
- New words are  added this class from time to time therefore it is called open class.

##### Closed class words 
-  Pronouns, Determiners, Prepositions, Connectives.
- Limited number of words present in this class.
- They are most functional words: to tie the concepts of a sentence together.

###### POS Examples
![[Pasted image 20230609162912.png]]


-  For tagging sentence we have to first define a tag set.  More commonly used set if finer grained, **UPenn TreeBank tag set** containing about 45 tags.
![[Pasted image 20230609163330.png]]

**Example Sentence:**
The grand jury commented on a number of other topics.

**POS tagged sentence:**
The/DT grand/JJ jury/NN commented/VBD on/IN a/DT number/NN of/IN other/JJ topics/NNS


### <font color = " green"> Why is POS tagging hard?</font>
-  Words usually have more than one POS tags so you  cannot tag a word in lexicon with a single part of speech tag.
Example:
![[Pasted image 20230609163754.png]]

##### How common is this problem?
In the [Brown Corpus](https://en.wikipedia.org/wiki/Brown_Corpus)
-  40% of  word tokens and 12% of word types are ambiguous i.e. you cannot assign a single tag to that word / token.
![[Pasted image 20230609165031.png]]

### <font color = "green"> How bad is the ambiguity problem? </font> 
-  One tag is more likely to occur than the other.
- In the Brown corpus, $race$  is a noun 98% of the time, and a verb 2% of the time.
- A tagger for English that simply chooses the most likely tag for each word can achieve good performance.
- Any new approach should be compared against the unigram baseline (assigning each token to its most likely tag).


### Relevant knowledge for POS tagging
-  Words itself 
	- Some words may only be nouns, e.g. $arrow$.
	- Some words are ambiguous, e.g. $like$, $flies$.
-  Local Context 
	- Two determiners rarely follow each other.
	- Two base form verbs rarely follow each other.
	- Determiner is almost always followed by adjective or noun.


### Two Approaches for POS tagging

-  Rule based Approach
	- Assign each word in the input a list of potential POS tags.
	- Then narrow down this list to a single tag using the context.
- Statistical Tagging
	- Get a training corpus of tagged text, learn the transformation rules from the most frequent tags (TBL tagger).
	- Probabilistic: Find the most likely sequence of tags  $T$  for a sequence of words $W$

- **TBL Tagger**
![[Pasted image 20230609172823.png]]
In the above example $MD$  --> $NN$: $DT$ _   means Modal Verb transforms to Noun if it is preceded by a Determiner. $VBD$ --> $VBN$: $VBD$ _  means a Verb in past tense is transformed to a Verb in past participle if it is preceded by a Verb in past tense.


#### Probabilistic Tagging Model

![[Pasted image 20230609174113.png]]

#### Generative (Joint) Models
- Generate the observed data from the hidden classed i.e. put a probability over the observations given the class : P($d$, $c$) in terms of P($d$ | $c$).
- Suppose you have to write an essay on topic **Politics** then your essay will be the data generated from the class **Politics**.
-  e.g. Na√Øve Bayes classifiers, Hidden Markov Models etc.

#### Discriminative (Conditional) Models
-  Take the data as given, and put a probability over hidden structure given the data: 
  P($c$ | $d$) 
  - Suppose you read an essay and you give a single word summarizing the whole content of essay.
  -  e.g. Logistic regression, maximum entropy models, conditional random fields.
![[Pasted image 20230609174751.png]]


### Generative Models  $vs$  Discriminative  Models
![[Pasted image 20230609175425.png]]

-  A joint model gives probabilities P($d$, $c$) and tries to maximize this joint likelihood. 
- A conditional model gives probabilities P($c$ | $d$), taking the given data and modelling only the conditional probabilities of the class.