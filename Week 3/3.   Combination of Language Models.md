
-  The power (expressiveness) of an N-Gram model increases but the smoothing problem gets worse.
- A general approach to solve this problem is to combined the results of multiple N-gram models.
-  We take advantage of large N-gram and solve the sparseness problem by using small N-gram model and then combine their results.


#### Backoff Language Model

- Use Trigram if it occurs in the corpus otherwise  use (backoff to) Bigrams and Unigrams.

![[Pasted image 20230530214643.png]]

-  <font color = " Red">How to normalize the probabilities?</font>  

![[Pasted image 20230530220223.png]]

**$P_{bo}$**  --->  Backoff Probability. 
**$\hat{P}$** --->  Reduced Probability given to each word after some probability mass is stolen from their MLE.
**$\lambda$**  ---> Multiplicative Constant for making sure that the probability mass adds up to $1$. This constant is multiplied to the bigram backoff probability.
$c$  ---> Count of trigrams and bigrams in the corpus.

<font color = "Red">What is the meaning of</font>  $\hat{P}$  <font color = "Red">?</font>
Suppose we have three words $W_1 , W_2, W_3$  and there trigram probability is  $\frac{3}{5}, \frac{2}{5}, 0$  .  We cannot assign a bigram backoff probability because the trigram probability of $W_1$  and  $W_2$ add up to  $\frac{3}{5} + \frac{2}{5} = 1$ . So how can we assign a bigram backoff probability to the trigram probability of $W_3$ ?  We steal some probability mass from other trigrams by subtracting  a constant value. Do not confuse this with $\lambda$.

##### Example

![[Pasted image 20230530224314.png]]

Solution: 

-  Computing the trigram backoff probability
![[Pasted image 20230530225217.png]]

-  Computing the bigram backoff probability
  ![[Pasted image 20230530225449.png]]

<font color = "Red" >Now how to compute</font> $\lambda$ <font color = "Red"> ?</font>
-  We simply add and equate all the probabilities to $1$.
$$\lambda_b.[\frac{1}{8} + \frac{3}{32}] + \frac{4}{8} + \frac{2}{8} = 1$$
-  We get $\lambda_b = \frac{8}{7}$, this is the multiplicative constant used for computing the bigram backoff probability. Substitute this value and get the individual backoff probability for each word.

-  Substitute this backoff probability for the word with zero value in the trigrams and then compute $\lambda_{ab}$   the same way as before.



#### Interpolation of Language Model
-  Mix Unigram, Bigram and Trigram.

**Linear Interpolation**
![[Pasted image 20230530230937.png]]

**Condition lambdas on context**
![[Pasted image 20230530231046.png]]

**Setting the lambda values**
![[Pasted image 20230530231240.png]]

-  Interpolation usually works better than backoff but Stupid Backoff works better at larger scale.

Credit: [Andr√© Ribeiro Miranda](https://youtu.be/HInzCyXgqsI)

![[Pasted image 20230530232944.png]]